{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI in Networking: Theoretical Frameworks for ML-Based Traffic Prediction and Anomaly Detection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive Jupyter Notebook designed for aspiring scientists and researchers. As a blend of theoretical depth, practical implementation, and forward-thinking insights—inspired by the rigorous methodologies of Alan Turing, Albert Einstein, and Nikola Tesla—this notebook equips you to advance in AI-driven networking. We cover fundamentals to advanced topics, with code, visualizations, projects, and exercises.\n",
    "\n",
    "**Notebook Objectives:**\n",
    "- Build foundational knowledge.\n",
    "- Implement ML models hands-on.\n",
    "- Explore real-world applications and research frontiers.\n",
    "- Foster self-learning through exercises and projects.\n",
    "\n",
    "Run cells sequentially. Ensure libraries are installed: `pip install numpy pandas matplotlib scikit-learn tensorflow torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Theory & Tutorials – From Fundamentals to Advanced\n",
    "\n",
    "### 1.1 Fundamentals of Computer Networks\n",
    "\n",
    "Computer networks are systems that connect devices to share data, like roads connecting cities. Key components include:\n",
    "- **Nodes**: Devices (e.g., computers, routers).\n",
    "- **Links**: Connections (wired or wireless).\n",
    "- **Protocols**: Rules for communication (e.g., TCP/IP).\n",
    "\n",
    "The **OSI Model** has 7 layers:\n",
    "1. Physical: Hardware transmission.\n",
    "2. Data Link: Error detection.\n",
    "3. Network: Routing (e.g., IP addresses).\n",
    "4. Transport: Reliable delivery (e.g., TCP).\n",
    "5. Session: Managing connections.\n",
    "6. Presentation: Data formatting.\n",
    "7. Application: User interfaces.\n",
    "\n",
    "**Network Traffic**: Data flow measured in packets. Congestion occurs when traffic exceeds capacity, modeled as queues (e.g., M/M/1 queue: arrival rate λ, service rate μ, utilization ρ = λ/μ).\n",
    "\n",
    "### 1.2 Introduction to AI and Machine Learning in Networking\n",
    "\n",
    "**AI** simulates human intelligence; **ML** is a subset where models learn from data without explicit rules.\n",
    "\n",
    "- **Supervised Learning**: Labeled data (e.g., predict traffic with known patterns).\n",
    "- **Unsupervised Learning**: Unlabeled data (e.g., detect anomalies by clustering).\n",
    "\n",
    "In networking:\n",
    "- **Traffic Prediction**: Forecast future traffic using time-series models like ARIMA or LSTM.\n",
    "- **Anomaly Detection**: Identify unusual patterns (e.g., DDoS attacks) using autoencoders or isolation forests.\n",
    "\n",
    "**Theoretical Frameworks**: Structured approaches combining statistics, graph theory, and neural networks. For prediction, use spatio-temporal models; for detection, unsupervised learning on normal baselines.\n",
    "\n",
    "### 1.3 Advanced Concepts\n",
    "\n",
    "- **LSTM for Prediction**: Long Short-Term Memory networks handle sequential data with gates (input, forget, output) to mitigate vanishing gradients.\n",
    "- **Autoencoders for Detection**: Neural networks that compress (encode) and reconstruct data; high reconstruction error indicates anomalies.\n",
    "\n",
    "Math Insight: For LSTM, cell state update: $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$, where $\\odot$ is element-wise multiplication, and gates are sigmoids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Visualizations in Theory\n",
    "\n",
    "Let's visualize a simple network topology and traffic flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple visualization of network topology (Star Topology)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.add_patch(plt.Circle((0.5, 0.5), 0.1, color='red'))  # Central hub\n",
    "for i in range(4):\n",
    "    angle = i * 90\n",
    "    x = 0.5 + 0.3 * np.cos(np.radians(angle))\n",
    "    y = 0.5 + 0.3 * np.sin(np.radians(angle))\n",
    "    ax.add_patch(plt.Circle((x, y), 0.05, color='blue'))  # Nodes\n",
    "    ax.plot([0.5, x], [0.5, y], 'k--')  # Connections\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Star Network Topology')\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Simulated traffic flow plot\n",
    "time = np.arange(0, 24, 1)\n",
    "traffic = 50 + 30 * np.sin(2 * np.pi * time / 24) + np.random.normal(0, 10, 24)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(time, traffic)\n",
    "plt.title('Daily Network Traffic Pattern')\n",
    "plt.xlabel('Time (Hours)')\n",
    "plt.ylabel('Traffic Volume')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Practical Code Guides – Step-by-Step Implementation\n",
    "\n",
    "### 2.1 Traffic Prediction with LSTM\n",
    "\n",
    "We'll use synthetic data for demonstration (replace with real datasets like CESNET-TimeSeries24). Steps:\n",
    "1. Generate/ load time-series data.\n",
    "2. Scale and create sequences.\n",
    "3. Build and train LSTM model.\n",
    "4. Predict and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic traffic data (e.g., hourly for 1000 hours)\n",
    "np.random.seed(42)\n",
    "time_steps = 1000\n",
    "traffic = np.cumsum(np.random.randn(time_steps)) + 100  # Random walk with trend\n",
    "\n",
    "# Step 2: Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "traffic_scaled = scaler.fit_transform(traffic.reshape(-1, 1))\n",
    "\n",
    "# Create sequences (look back 10 steps to predict next)\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 10\n",
    "X, y = create_sequences(traffic_scaled, seq_length)\n",
    "\n",
    "# Split into train/test\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Step 3: Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Step 4: Train\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scale\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "print(f'Mean Squared Error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Anomaly Detection with Autoencoder\n",
    "\n",
    "Steps:\n",
    "1. Generate normal and anomalous data.\n",
    "2. Train autoencoder on normal data.\n",
    "3. Detect anomalies via reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic normal traffic (multivariate for realism)\n",
    "normal_data = np.random.normal(100, 10, (800, 5))  # 800 samples, 5 features\n",
    "anomaly_data = np.random.normal(200, 20, (200, 5))  # Anomalous\n",
    "all_data = np.vstack([normal_data, anomaly_data])\n",
    "\n",
    "# Step 2: Scale\n",
    "scaler_ae = MinMaxScaler()\n",
    "all_data_scaled = scaler_ae.fit_transform(all_data)\n",
    "normal_scaled = all_data_scaled[:800]\n",
    "\n",
    "# Step 3: Build Autoencoder\n",
    "input_dim = 5\n",
    "encoding_dim = 2\n",
    "\n",
    "autoencoder = Sequential([\n",
    "    Dense(encoding_dim, activation='relu', input_shape=(input_dim,)),\n",
    "    RepeatVector(input_dim),\n",
    "    TimeDistributed(Dense(encoding_dim, activation='relu')),\n",
    "    TimeDistributed(Dense(1, activation='sigmoid'))\n",
    "])\n",
    "autoencoder.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "\n",
    "# Train on normal data\n",
    "autoencoder.fit(normal_scaled, normal_scaled, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Step 4: Detect anomalies\n",
    "reconstructions = autoencoder.predict(all_data_scaled)\n",
    "mse_ae = np.mean(np.power(all_data_scaled - reconstructions, 2), axis=1)\n",
    "\n",
    "# Threshold (mean + 3*std of normal errors)\n",
    "threshold = np.mean(mse_ae[:800]) + 3 * np.std(mse_ae[:800])\n",
    "anomalies = mse_ae > threshold\n",
    "\n",
    "print(f'Detected {np.sum(anomalies)} anomalies out of {len(anomalies)} samples.')\n",
    "print(f'Threshold: {threshold:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualizations – Diagrams, Plots, and Representations\n",
    "\n",
    "Visuals aid understanding. Here, we plot training history, predictions, and anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('LSTM Training History')\n",
    "plt.legend()\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_test_inv[:100], label='Actual')\n",
    "plt.plot(y_pred_inv[:100], label='Predicted')\n",
    "plt.title('Traffic Prediction (First 100 Test Points)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Anomaly visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(mse_ae, label='Reconstruction Error')\n",
    "plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Anomaly Detection Scores')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Applications – Real-World Examples\n",
    "\n",
    "AI in networking optimizes resources and enhances security.\n",
    "\n",
    "- **Traffic Prediction**: In 5G networks (e.g., SK Telecom), LSTM predicts peaks for dynamic bandwidth allocation, reducing latency by 20-30%.\n",
    "- **Anomaly Detection**: Cisco uses ML to detect DDoS in enterprise networks, preventing breaches in real-time.\n",
    "\n",
    "Dataset Example: Use CESNET-TimeSeries24 (2025 dataset for anomaly detection and forecasting) or Kaggle's Network Traffic Anomaly Detection Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Research Directions & Rare Insights\n",
    "\n",
    "**Current Trends (2024-2025)**:\n",
    "- Integration of Graph Neural Networks (GNNs) with LSTMs for spatio-temporal prediction.\n",
    "- Federated Learning for privacy-preserving anomaly detection in distributed networks.\n",
    "\n",
    "**Rare Insights**:\n",
    "- Quantum ML could enable ultra-fast predictions in 6G, but challenges include noise in quantum states (inspired by Tesla's AC systems for efficiency).\n",
    "- Ethical Bias: Models may flag legitimate traffic from underrepresented regions as anomalies—address via diverse datasets.\n",
    "\n",
    "From recent studies: Adaptive ML for real-time WAN anomaly detection (EPJ Conferences, 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Mini & Major Projects\n",
    "\n",
    "### Mini Project: Simple Traffic Prediction on Kaggle Dataset\n",
    "\n",
    "Load Kaggle Network Traffic Dataset and apply LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume dataset downloaded as 'network_traffic.csv' with 'traffic_volume' column\n",
    "# df = pd.read_csv('network_traffic.csv')\n",
    "# For demo, use synthetic\n",
    "print('Download from Kaggle: https://www.kaggle.com/datasets/ravikumargattu/network-traffic-dataset')\n",
    "print('Then: traffic_col = df[\"traffic_volume\"].values')\n",
    "print('Apply scaling and LSTM as in Section 2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Project: Anomaly Detection on NSL-KDD Dataset\n",
    "\n",
    "Use NSL-KDD for intrusion detection. Train autoencoder and evaluate precision/recall.\n",
    "\n",
    "Steps: Load data, preprocess features, train, detect attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for major project (download NSL-KDD from https://www.unb.ca/cic/datasets/nsl.html)\n",
    "# df = pd.read_csv('KDDTrain+.csv', header=None)\n",
    "# Features: df.iloc[:, :-1], labels: df.iloc[:, -1]\n",
    "# Normal data only for training\n",
    "# normal_df = df[df.iloc[:, -1] == 'normal']\n",
    "# Scale and train autoencoder as in 2.2\n",
    "# Compute errors on full data, flag high errors as anomalies\n",
    "print('Implement full pipeline; evaluate with classification metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Exercises – For Self-Learning\n",
    "\n",
    "### Exercise 1: Modify LSTM for Multi-Step Prediction\n",
    "Change the model to predict 5 steps ahead. Solution: Adjust output Dense to 5 units, reshape y accordingly.\n",
    "\n",
    "### Exercise 2: Tune Autoencoder Hyperparameters\n",
    "Try different encoding dimensions (1-4). Which minimizes false positives? Solution: Loop over dims, compute F1-score on test set.\n",
    "\n",
    "**Solutions (Hidden in Practice):** Run the following for Ex1 demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1 (Multi-step)\n",
    "# Modify create_sequences for multi-output\n",
    "def create_multi_sequences(data, seq_length, steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - steps):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_multi, y_multi = create_multi_sequences(traffic_scaled, 10, 5)\n",
    "# Then Dense(5) in model\n",
    "print('Adapted for multi-step prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Future Directions & Next Steps\n",
    "\n",
    "- **Study Advanced Topics**: Explore GNNs for graph-based networks (Keras example: timeseries_traffic_forecasting).\n",
    "- **Research Paths**: Publish on arXiv; focus on 6G anomaly detection.\n",
    "- **Next Steps**: Implement on real hardware (e.g., Mininet simulator); join IEEE conferences.\n",
    "\n",
    "Trends: Deep learning for encrypted traffic (arXiv 2025 surveys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: What’s Missing in Standard Tutorials\n",
    "\n",
    "Standard tutorials overlook:\n",
    "- **Scalability**: Handling petabyte-scale data with distributed ML (e.g., Spark integration).\n",
    "- **Explainability**: Use SHAP to interpret LSTM decisions.\n",
    "- **Ethics & Bias**: Audit datasets for fairness.\n",
    "- **Hybrid Models**: Combine LSTM with Isolation Forest for robust systems.\n",
    "\n",
    "Insight: As Einstein pondered relativity's implications, consider AI's societal impact in networking security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete foundation. Experiment, iterate, and innovate—like Turing's universal machine, your contributions can redefine networking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}